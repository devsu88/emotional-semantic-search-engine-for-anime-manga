{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9977156",
   "metadata": {
    "papermill": {
     "duration": 0.005865,
     "end_time": "2025-04-19T09:39:49.540270",
     "exception": false,
     "start_time": "2025-04-19T09:39:49.534405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Gen AI Intensive Course Capstone 2025Q1: Advanced Emotional Semantic Search Engine for Anime/Manga (RAG + ChromaDB + Re-ranking)\n",
    "\n",
    "*   **The Challenge:** Discovering new anime or manga often goes beyond simple genre tags or keyword searches. Users frequently look for titles based on a specific *feeling*, *atmosphere*, *vibe*, or inspiration derived from past experiences (e.g., \"something melancholic and beautiful like *Your Name*\", \"an epic adventure that gets you hyped like *Gurren Lagann*\"). Traditional search methods struggle to capture these nuanced, emotional queries.\n",
    "\n",
    "*   **Our Enhanced Solution: A Sophisticated RAG Pipeline:** This project tackles the challenge by implementing an advanced **Retrieval-Augmented Generation (RAG)** system specifically designed for vibe-based anime/manga discovery. The pipeline involves several key stages:\n",
    "    1.  **Data Loading & Filtering:** We start with datasets containing anime information (synopses, genres, scores, popularity, etc.) and user reviews. Crucially, we apply **genre filtering** upfront to exclude categories that might skew recommendations, ensuring a more relevant starting pool.\n",
    "    2.  **Preprocessing:** Text data (synopses, reviews) is cleaned and prepared for processing.\n",
    "    3.  **Semantic Embedding:** We leverage Google's powerful `text-embedding-004` model via the `google-genai` library to generate rich **semantic embeddings** for the anime synopses, capturing their underlying meaning.\n",
    "    4.  **Vector Storage & Indexing:** Instead of in-memory arrays, we utilize **ChromaDB**, a persistent **vector database**, to efficiently store and index the anime embeddings along with relevant metadata (title, genre, score, popularity). This allows for persistence across sessions and faster retrieval.\n",
    "    5.  **Hybrid Retrieval & Re-ranking:** When a user query is received:\n",
    "        *   A **vector search** is performed in ChromaDB to retrieve an initial set of `k_initial` candidates based on semantic similarity between the query and anime synopses.\n",
    "        *   A crucial **re-ranking** step is applied to these candidates. We calculate a combined score based on weighted factors: **semantic similarity**, **user score** (quality), and **popularity rank** (inverse rank, lower is better). This allows high-quality or popular relevant items to surface even if they aren't the absolute closest semantically.\n",
    "    6.  **Context Augmentation:** The top `k` re-ranked anime are then augmented with additional context: their detailed **synopses** and relevant **user review snippets**, providing qualitative insights into the perceived vibe and emotional impact.\n",
    "    7.  **Motivated Generation:** Finally, Google's **Gemini LLM** is prompted with the original user query and the rich, augmented context (re-ranked results, synopses, metadata, reviews). The LLM synthesizes this information to generate a personalized, **motivated recommendation**, explaining *why* the suggested titles might match the user's desired feeling.\n",
    "\n",
    "\n",
    "*   **Notebook Objective:** This notebook provides a comprehensive, step-by-step implementation of this advanced RAG pipeline. It covers data loading, filtering, preprocessing, embedding generation, ChromaDB integration, the hybrid retrieval/re-ranking mechanism, review augmentation, LLM-based generation, and includes a section on evaluating the system's performance.\n",
    "\n",
    "*   **Key Technologies & Gen AI Capabilities Demonstrated:**\n",
    "    *   **Retrieval-Augmented Generation (RAG):** The core architecture.\n",
    "    *   **Semantic Embeddings:** Using `google-genai`'s `text-embedding-004`.\n",
    "    *   **Vector Database / Vector Search:** Implementation with `chromadb`.\n",
    "    *   **LLM Integration:** Using `google-genai`'s `gemini-2.0-flash` for response generation.\n",
    "    *   **Re-ranking Algorithms:** Combining semantic similarity with metadata features.\n",
    "    *   **Data Preprocessing & Filtering:** Essential steps for quality results.\n",
    "    *   **Context Augmentation:** Using reviews and metadata to enrich LLM input.\n",
    "    *   **Libraries:** `pandas`, `numpy`, `google-genai`, `chromadb`, `scikit-learn`.\n",
    "\n",
    "This updated introduction sets the stage more accurately, highlighting the specific techniques and the flow of your sophisticated recommendation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8659a6a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:39:49.552575Z",
     "iopub.status.busy": "2025-04-19T09:39:49.552084Z",
     "iopub.status.idle": "2025-04-19T09:39:49.674813Z",
     "shell.execute_reply": "2025-04-19T09:39:49.673349Z"
    },
    "papermill": {
     "duration": 0.13134,
     "end_time": "2025-04-19T09:39:49.677011",
     "exception": false,
     "start_time": "2025-04-19T09:39:49.545671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf /kaggle/working/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9795bab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:39:49.688820Z",
     "iopub.status.busy": "2025-04-19T09:39:49.688422Z",
     "iopub.status.idle": "2025-04-19T09:40:32.902012Z",
     "shell.execute_reply": "2025-04-19T09:40:32.900577Z"
    },
    "papermill": {
     "duration": 43.22242,
     "end_time": "2025-04-19T09:40:32.904486",
     "exception": false,
     "start_time": "2025-04-19T09:39:49.682066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping kfp as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.9/454.9 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "google-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "pandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\r\n",
      "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -qqy jupyterlab kfp  # Remove unused conflicting packages\n",
    "!pip install -qU \"google-genai==1.7.0\" \"chromadb==0.6.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "732cb11f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:40:32.920065Z",
     "iopub.status.busy": "2025-04-19T09:40:32.919678Z",
     "iopub.status.idle": "2025-04-19T09:40:34.200869Z",
     "shell.execute_reply": "2025-04-19T09:40:34.199778Z"
    },
    "papermill": {
     "duration": 1.291049,
     "end_time": "2025-04-19T09:40:34.202709",
     "exception": false,
     "start_time": "2025-04-19T09:40:32.911660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "genai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2180fd2d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-19T09:40:34.219220Z",
     "iopub.status.busy": "2025-04-19T09:40:34.218605Z",
     "iopub.status.idle": "2025-04-19T09:40:35.107062Z",
     "shell.execute_reply": "2025-04-19T09:40:35.105938Z"
    },
    "papermill": {
     "duration": 0.898566,
     "end_time": "2025-04-19T09:40:35.108808",
     "exception": false,
     "start_time": "2025-04-19T09:40:34.210242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/myanimelist-dataset-animes-profiles-reviews/animes.csv\n",
      "/kaggle/input/myanimelist-dataset-animes-profiles-reviews/profiles.csv\n",
      "/kaggle/input/myanimelist-dataset-animes-profiles-reviews/reviews.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760357ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:40:35.125020Z",
     "iopub.status.busy": "2025-04-19T09:40:35.124425Z",
     "iopub.status.idle": "2025-04-19T09:40:37.970342Z",
     "shell.execute_reply": "2025-04-19T09:40:37.968917Z"
    },
    "papermill": {
     "duration": 2.856151,
     "end_time": "2025-04-19T09:40:37.972301",
     "exception": false,
     "start_time": "2025-04-19T09:40:35.116150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2. Environment Setup ---\n",
      "Google AI SDK Configured successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 2. Environment Setup\n",
    "# ==============================================================================\n",
    "print(\"--- 2. Environment Setup ---\")\n",
    "from google.api_core import retry # For handling temporary API errors\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import textwrap # For formatting LLM output\n",
    "import re\n",
    "import warnings\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import ast\n",
    "from sklearn.preprocessing import MinMaxScaler # For normalization\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") # Hide non-critical warnings\n",
    "\n",
    "# Configure API Key from Kaggle Secrets\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    GOOGLE_API_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n",
    "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "    print(\"Google AI SDK Configured successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring Google AI SDK: {e}\")\n",
    "    print(\"Make sure you have added your API Key as 'GOOGLE_API_KEY' in Kaggle Secrets.\")\n",
    "    GOOGLE_API_KEY = None # Set to None for later checks\n",
    "\n",
    "# Global Parameters\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-004\"\n",
    "GENERATIVE_MODEL_NAME = \"gemini-2.0-flash\" #\"gemini-1.5-flash-latest\"\n",
    "TOP_K_RESULTS = 10 # Number of documents to retrieve for the LLM\n",
    "REVIEWS_PER_ANIME = 3 # Number of review snippets to use for augmentation\n",
    "MIN_SYNOPSIS_LENGTH = 250 # Minimum synopsis length in characters\n",
    "MIN_REVIEW_LENGTH = 30 # Minimum review length in characters\n",
    "\n",
    "ANIME_DATA_PATH = \"/kaggle/input/myanimelist-dataset-animes-profiles-reviews/animes.csv\"\n",
    "REVIEWS_DATA_PATH = \"/kaggle/input/myanimelist-dataset-animes-profiles-reviews/reviews.csv\"\n",
    "\n",
    "PATH_PROCESSED_ANIME = \"/kaggle/working/processed_anime.csv\"\n",
    "PATH_PROCESSED_REVIEWS = \"/kaggle/working/processed_reviews.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "440b78d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:40:37.989011Z",
     "iopub.status.busy": "2025-04-19T09:40:37.988358Z",
     "iopub.status.idle": "2025-04-19T09:40:38.778439Z",
     "shell.execute_reply": "2025-04-19T09:40:38.776867Z"
    },
    "papermill": {
     "duration": 0.800447,
     "end_time": "2025-04-19T09:40:38.780330",
     "exception": false,
     "start_time": "2025-04-19T09:40:37.979883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anime Dataset loaded: 19311 rows, 12 columns.\n",
      "Anime Columns: ['uid', 'title', 'synopsis', 'genre', 'aired', 'episodes', 'members', 'popularity', 'ranked', 'score', 'img_url', 'link']\n",
      "\n",
      "Extracting unique genres from the dataset...\n",
      "\n",
      "Found 43 unique genres:\n",
      "Action, Adventure, Cars, Comedy, Dementia\n",
      "Demons, Drama, Ecchi, Fantasy, Game\n",
      "Harem, Hentai, Historical, Horror, Josei\n",
      "Kids, Magic, Martial Arts, Mecha, Military\n",
      "Music, Mystery, Parody, Police, Psychological\n",
      "Romance, Samurai, School, Sci-Fi, Seinen\n",
      "Shoujo, Shoujo Ai, Shounen, Shounen Ai, Slice of Life\n",
      "Space, Sports, Super Power, Supernatural, Thriller\n",
      "Vampire, Yaoi, Yuri\n",
      "\n",
      "Use the list above to define the 'EXCLUDED_GENRES' set in the next step.\n"
     ]
    }
   ],
   "source": [
    "# --- Extract and Display Unique Genres (NEW STEP) ---\n",
    "try:\n",
    "    df_anime = pd.read_csv(ANIME_DATA_PATH)\n",
    "    print(f\"Anime Dataset loaded: {df_anime.shape[0]} rows, {df_anime.shape[1]} columns.\")\n",
    "    print(\"Anime Columns:\", df_anime.columns.tolist())\n",
    "\n",
    "    if not df_anime.empty and 'genre' in df_anime.columns:\n",
    "        print(\"\\nExtracting unique genres from the dataset...\")\n",
    "        all_genres = set()\n",
    "    \n",
    "        # Handle potential NaN values before attempting to parse genres\n",
    "        df_anime['genre'].dropna(inplace=True) # Drop rows where genre is NaN\n",
    "    \n",
    "        # Iterate through the genre column to parse and collect unique genres\n",
    "        for genre_input in df_anime['genre']:\n",
    "            genre_list = []\n",
    "            if isinstance(genre_input, str):\n",
    "                try:\n",
    "                    # Attempt 1: Assume string representation of list like \"['Action', 'Sci-Fi']\"\n",
    "                    parsed_list = ast.literal_eval(genre_input)\n",
    "                    if isinstance(parsed_list, list):\n",
    "                        genre_list = parsed_list\n",
    "                    else:\n",
    "                        genre_list = [str(parsed_list)] # Treat as single item list\n",
    "                except (ValueError, SyntaxError):\n",
    "                    # Attempt 2: Assume comma-separated string like \"Action, Sci-Fi\"\n",
    "                    genre_list = [g.strip() for g in genre_input.split(',') if g.strip()]\n",
    "            elif isinstance(genre_input, list): # If data is already a list\n",
    "                 genre_list = genre_input\n",
    "            # else: Ignore other types for genre extraction\n",
    "    \n",
    "            # Add valid genres to the set\n",
    "            for genre in genre_list:\n",
    "                if isinstance(genre, str) and genre.strip(): # Ensure it's a non-empty string\n",
    "                    all_genres.add(genre.strip())\n",
    "    \n",
    "        # Sort the unique genres alphabetically for readability\n",
    "        unique_genres_sorted = sorted(list(all_genres))\n",
    "    \n",
    "        print(f\"\\nFound {len(unique_genres_sorted)} unique genres:\")\n",
    "        # Print genres in multiple columns for better readability if the list is long\n",
    "        genres_per_line = 5\n",
    "        for i in range(0, len(unique_genres_sorted), genres_per_line):\n",
    "            print(\", \".join(unique_genres_sorted[i:i+genres_per_line]))\n",
    "    \n",
    "        print(\"\\nUse the list above to define the 'EXCLUDED_GENRES' set in the next step.\")\n",
    "    \n",
    "    elif 'genre' not in df_anime.columns:\n",
    "         print(\"Warning: 'genre' column not found in the anime dataset. Cannot extract unique genres.\")\n",
    "    else:\n",
    "         print(\"Anime dataset is empty. Cannot extract unique genres.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Anime dataset file not found at {ANIME_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3080a28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:40:38.797351Z",
     "iopub.status.busy": "2025-04-19T09:40:38.796946Z",
     "iopub.status.idle": "2025-04-19T09:40:38.801254Z",
     "shell.execute_reply": "2025-04-19T09:40:38.800236Z"
    },
    "papermill": {
     "duration": 0.014763,
     "end_time": "2025-04-19T09:40:38.803038",
     "exception": false,
     "start_time": "2025-04-19T09:40:38.788275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the set of genres to exclude (CASE-SENSITIVE! Match exact names in your data)\n",
    "# Adjust this set based on the genres you want to remove\n",
    "EXCLUDED_GENRES = {'Hentai', 'Yaoi', 'Yuri'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60f575f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:40:38.819895Z",
     "iopub.status.busy": "2025-04-19T09:40:38.819349Z",
     "iopub.status.idle": "2025-04-19T09:42:06.885609Z",
     "shell.execute_reply": "2025-04-19T09:42:06.884115Z"
    },
    "papermill": {
     "duration": 88.076962,
     "end_time": "2025-04-19T09:42:06.887605",
     "exception": false,
     "start_time": "2025-04-19T09:40:38.810643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Data Loading and Preparation ---\n",
      "Anime Dataset loaded: 19311 rows, 12 columns.\n",
      "Anime Columns: ['uid', 'title', 'synopsis', 'genre', 'aired', 'episodes', 'members', 'popularity', 'ranked', 'score', 'img_url', 'link']\n",
      "\n",
      "Filtering Anime based on excluded genres...\n",
      "Excluding genres: {'Yuri', 'Yaoi', 'Hentai'}\n",
      "Applying genre filter function...\n",
      "Removed 2640 anime containing excluded genres.\n",
      "Anime remaining after genre filtering: 16671\n",
      "Reviews Dataset loaded: 192112 rows, 7 columns.\n",
      "Reviews Columns: ['uid', 'profile', 'anime_uid', 'text', 'score', 'scores', 'link']\n",
      "\n",
      "Preprocessing Filtered Anime Dataset...\n",
      "Anime: Removed 729 rows with missing synopsis (post-genre filter).\n",
      "Anime: Removed 7010 rows with synopsis shorter than 250 characters (post-genre filter).\n",
      "Processed Anime Dataset: 7686 anime ready.\n",
      "        uid                                        title  \\\n",
      "1127  40911                          Yuukoku no Moriarty   \n",
      "1164  40908                                 Kemono Jihen   \n",
      "6667  40839                         Kanojo, Okarishimasu   \n",
      "1118  40938  Hige wo Soru. Soshite Joshikousei wo Hirou.   \n",
      "6857  40906        Dragon Quest: Dai no Daibouken (2020)   \n",
      "\n",
      "                                       cleaned_synopsis  \n",
      "1127  everyone is familiar with the story of sherloc...  \n",
      "1164  when a series of animal bodies that rot away a...  \n",
      "6667  dumped by his girlfriend, emotionally shattere...  \n",
      "1118  office worker yoshida has been crushing on his...  \n",
      "6857  a long time ago, there was a valiant swordsman...  \n",
      "        uid                                        title  \\\n",
      "1127  40911                          Yuukoku no Moriarty   \n",
      "1164  40908                                 Kemono Jihen   \n",
      "6667  40839                         Kanojo, Okarishimasu   \n",
      "1118  40938  Hige wo Soru. Soshite Joshikousei wo Hirou.   \n",
      "6857  40906        Dragon Quest: Dai no Daibouken (2020)   \n",
      "\n",
      "                                                  genre  \n",
      "1127  ['Mystery', 'Historical', 'Psychological', 'Sh...  \n",
      "1164  ['Action', 'Mystery', 'Demons', 'Supernatural'...  \n",
      "6667         ['Comedy', 'Romance', 'School', 'Shounen']  \n",
      "1118                               ['Drama', 'Romance']  \n",
      "6857      ['Action', 'Adventure', 'Fantasy', 'Shounen']  \n",
      "\n",
      "Preprocessing Reviews Dataset...\n",
      "Reviews: Removed 0 rows with missing text.\n",
      "Reviews: Removed 0 rows with review shorter than 30 characters.\n",
      "Reviews: Kept 179143 reviews related to anime in the processed dataset (removed 12969).\n",
      "Reviews: Kept 179143 reviews related to anime in the *filtered* processed dataset (removed 0).\n",
      "Processed Reviews Dataset: 121487 reviews ready.\n",
      "   anime_uid                                     cleaned_review\n",
      "0      34096  more pics overall 8 story 8 animation 8 sound ...\n",
      "1      34599  more pics overall 10 story 10 animation 10 sou...\n",
      "2      28891  more pics overall 7 story 7 animation 9 sound ...\n",
      "3       2904  more pics overall 9 story 9 animation 9 sound ...\n",
      "4       4181  more pics overall 10 story 10 animation 8 soun...\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3. Data Loading and Preparation (Anime and Reviews)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 3. Data Loading and Preparation ---\")\n",
    "\n",
    "if os.path.exists(PATH_PROCESSED_ANIME) and os.path.exists(PATH_PROCESSED_REVIEWS):\n",
    "    print(f\"Skipping preprocessing step...\")\n",
    "    df_processed = pd.read_csv(PATH_PROCESSED_ANIME)\n",
    "    print(f\"Processed Anime Dataset: {df_processed.shape[0]} anime ready.\")\n",
    "    print(df_processed[['uid', 'title', 'cleaned_synopsis']].head())\n",
    "    \n",
    "    df_reviews_processed = pd.read_csv(PATH_PROCESSED_REVIEWS)\n",
    "    print(f\"Processed Reviews Dataset: {df_reviews_processed.shape[0]} reviews ready.\")\n",
    "    print(df_reviews_processed[['anime_uid', 'cleaned_review']].head())\n",
    "else:\n",
    "    # --- 3.1 Load Anime Dataset ---\n",
    "    try:\n",
    "        df_anime = pd.read_csv(ANIME_DATA_PATH)\n",
    "        print(f\"Anime Dataset loaded: {df_anime.shape[0]} rows, {df_anime.shape[1]} columns.\")\n",
    "        print(\"Anime Columns:\", df_anime.columns.tolist())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Anime dataset file not found at {ANIME_DATA_PATH}\")\n",
    "        df_anime = pd.DataFrame() # Create empty dataframe to avoid subsequent errors\n",
    "\n",
    "    # --- 3.1.1 Genre Filtering (NEW STEP) ---\n",
    "    if not df_anime.empty:\n",
    "        print(\"\\nFiltering Anime based on excluded genres...\")\n",
    "        \n",
    "        print(f\"Excluding genres: {EXCLUDED_GENRES}\")\n",
    "    \n",
    "        # Handle potential NaN values in the genre column before processing\n",
    "        # Option 1: Drop rows with NaN genres\n",
    "        df_anime.dropna(subset=['genre'], inplace=True)\n",
    "        # Option 2: Fill NaN with an empty list representation (if you prefer to keep them)\n",
    "        # df_anime['genre'].fillna('[]', inplace=True) # Use '[]' if using literal_eval, '' if using split\n",
    "    \n",
    "        original_rows_before_genre_filter = len(df_anime)\n",
    "    \n",
    "        # Function to parse the genre string and check for excluded genres\n",
    "        def check_and_filter_genres(genre_input, excluded_set):\n",
    "            if not genre_input or pd.isna(genre_input):\n",
    "                return False # Keep if genre is missing or NaN (already handled by dropna above ideally)\n",
    "    \n",
    "            genre_list = []\n",
    "            if isinstance(genre_input, str):\n",
    "                try:\n",
    "                    # Attempt 1: Assume string representation of list like \"['Action', 'Sci-Fi']\"\n",
    "                    parsed_list = ast.literal_eval(genre_input)\n",
    "                    if isinstance(parsed_list, list):\n",
    "                        genre_list = parsed_list\n",
    "                    else:\n",
    "                        # If literal_eval results in something else (e.g., just a string), treat as single item list\n",
    "                        genre_list = [str(parsed_list)]\n",
    "                except (ValueError, SyntaxError):\n",
    "                    # Attempt 2: Assume comma-separated string like \"Action, Sci-Fi\"\n",
    "                    # This handles cases like \"Action\" or \"Action, Comedy\"\n",
    "                    genre_list = [g.strip() for g in genre_input.split(',') if g.strip()]\n",
    "            elif isinstance(genre_input, list): # If data is already a list\n",
    "                 genre_list = genre_input\n",
    "            else:\n",
    "                 # Handle other potential types if necessary\n",
    "                 # print(f\"Warning: Unexpected genre type: {type(genre_input)}, value: {genre_input}\")\n",
    "                 return False # Keep if type is unexpected\n",
    "    \n",
    "            # Ensure we have a list of strings now\n",
    "            if not isinstance(genre_list, list):\n",
    "                 # print(f\"Warning: Could not parse genre input into a list: {genre_input}\")\n",
    "                 return False # Keep if parsing failed\n",
    "    \n",
    "            # Check for intersection with excluded genres using sets for efficiency\n",
    "            current_genres_set = set(str(g) for g in genre_list) # Ensure all items are strings\n",
    "            if not current_genres_set.isdisjoint(excluded_set): # isdisjoint is False if there is overlap\n",
    "                return True # Exclude this anime (it has an excluded genre)\n",
    "            else:\n",
    "                return False # Keep this anime\n",
    "    \n",
    "        # Apply the function to create a boolean mask for rows TO EXCLUDE\n",
    "        # It's safer to iterate using apply for robust parsing logic per row\n",
    "        print(\"Applying genre filter function...\")\n",
    "        exclude_mask = df_anime['genre'].apply(lambda x: check_and_filter_genres(x, EXCLUDED_GENRES))\n",
    "    \n",
    "        # Keep rows where the mask is False (i.e., do NOT exclude)\n",
    "        df_anime_filtered = df_anime[~exclude_mask].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "    \n",
    "        print(f\"Removed {original_rows_before_genre_filter - len(df_anime_filtered)} anime containing excluded genres.\")\n",
    "        print(f\"Anime remaining after genre filtering: {len(df_anime_filtered)}\")\n",
    "    \n",
    "        # Use the filtered DataFrame for subsequent steps\n",
    "        df_anime_to_process = df_anime_filtered\n",
    "        \n",
    "    else:\n",
    "        print(\"Anime Dataset is empty, skipping genre filtering.\")\n",
    "        df_anime_to_process = pd.DataFrame() # Ensure it's an empty DF\n",
    "    \n",
    "    # --- 3.2 Load Reviews Dataset ---\n",
    "    try:\n",
    "        df_reviews = pd.read_csv(REVIEWS_DATA_PATH)\n",
    "        print(f\"Reviews Dataset loaded: {df_reviews.shape[0]} rows, {df_reviews.shape[1]} columns.\")\n",
    "        print(\"Reviews Columns:\", df_reviews.columns.tolist())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Reviews dataset file not found at {REVIEWS_DATA_PATH}\")\n",
    "        df_reviews = pd.DataFrame() # Create empty dataframe\n",
    "    \n",
    "    # --- 3.3 Text Cleaning Function ---\n",
    "    def clean_text(text):\n",
    "        \"\"\"Cleans text: lowercase, removes specific tags, extra spaces.\"\"\"\n",
    "        if isinstance(text, str):\n",
    "            text = text.lower()\n",
    "            text = re.sub(r'\\[written by mal rewrite\\]', '', text, flags=re.IGNORECASE) # Remove common boilerplate\n",
    "            text = re.sub(r'<.*?>', '', text) # Remove HTML tags (if any)\n",
    "            text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # Remove URLs\n",
    "            text = re.sub(r'\\s+', ' ', text).strip() # Normalize whitespace\n",
    "            # Add other rules if needed (e.g., removing specific special characters)\n",
    "        return text\n",
    "    \n",
    "    # --- 3.4 Preprocessing Filtered Anime Dataset --- (Modify to use df_anime_to_process)\n",
    "    df_processed = pd.DataFrame() # Initialize empty processed dataframe\n",
    "    # Use the filtered dataframe 'df_anime_to_process' from now on\n",
    "    if not df_anime_to_process.empty:\n",
    "        print(\"\\nPreprocessing Filtered Anime Dataset...\")\n",
    "        # Select relevant columns from the filtered dataframe\n",
    "        cols_to_keep = ['uid', 'title', 'synopsis', 'genre', 'score', 'popularity'] # Add others if useful\n",
    "        df_processed = df_anime_to_process[cols_to_keep].copy()\n",
    "    \n",
    "        # Handle missing values in synopsis (on the filtered data)\n",
    "        original_rows = len(df_processed)\n",
    "        df_processed.dropna(subset=['synopsis'], inplace=True)\n",
    "        print(f\"Anime: Removed {original_rows - len(df_processed)} rows with missing synopsis (post-genre filter).\")\n",
    "    \n",
    "        # Clean synopsis\n",
    "        df_processed['cleaned_synopsis'] = df_processed['synopsis'].apply(clean_text)\n",
    "    \n",
    "        # Filter out very short synopses\n",
    "        original_rows = len(df_processed)\n",
    "        df_processed = df_processed[df_processed['cleaned_synopsis'].str.len() >= MIN_SYNOPSIS_LENGTH]\n",
    "        print(f\"Anime: Removed {original_rows - len(df_processed)} rows with synopsis shorter than {MIN_SYNOPSIS_LENGTH} characters (post-genre filter).\")\n",
    "    \n",
    "        # Handle 'uid' type\n",
    "        try:\n",
    "            df_processed['uid'] = pd.to_numeric(df_processed['uid'], errors='coerce')\n",
    "            df_processed.dropna(subset=['uid'], inplace=True)\n",
    "            df_processed['uid'] = df_processed['uid'].astype(int)\n",
    "        except Exception as e:\n",
    "             print(f\"Warning: Problem converting 'uid' in df_processed to numeric: {e}\")\n",
    "    \n",
    "        # Reset index\n",
    "        df_processed.reset_index(drop=True, inplace=True)\n",
    "        df_processed = df_processed.sort_values(\"popularity\", ascending=False).drop_duplicates(subset=[\"uid\"], keep=\"first\")\n",
    "        df_processed.to_csv(PATH_PROCESSED_ANIME, index=False)\n",
    "        print(f\"Processed Anime Dataset: {df_processed.shape[0]} anime ready.\")\n",
    "        print(df_processed[['uid', 'title', 'cleaned_synopsis']].head())\n",
    "        if not df_processed.empty:\n",
    "            print(df_processed[['uid', 'title', 'genre']].head()) # Show genres to verify filtering\n",
    "    else:\n",
    "        print(\"No anime data available after genre filtering to proceed with preprocessing.\")\n",
    "    \n",
    "    # --- 3.5 Preprocessing Reviews Dataset ---\n",
    "    df_reviews_processed = pd.DataFrame() # Initialize empty processed dataframe\n",
    "    if not df_reviews.empty and not df_processed.empty: # Proceed only if both dfs are loaded\n",
    "        print(\"\\nPreprocessing Reviews Dataset...\")\n",
    "        # Select relevant columns (use the provided names)\n",
    "        cols_to_keep_reviews = ['anime_uid', 'text', 'score'] # 'profile', 'scores', 'link' might be useful for future analysis\n",
    "        df_reviews_processed = df_reviews[cols_to_keep_reviews].copy()\n",
    "    \n",
    "        # Handle missing values in review text\n",
    "        original_rows = len(df_reviews_processed)\n",
    "        df_reviews_processed.dropna(subset=['text'], inplace=True)\n",
    "        print(f\"Reviews: Removed {original_rows - len(df_reviews_processed)} rows with missing text.\")\n",
    "    \n",
    "        # Clean review text\n",
    "        df_reviews_processed['cleaned_review'] = df_reviews_processed['text'].apply(clean_text)\n",
    "    \n",
    "        # Filter out very short reviews\n",
    "        original_rows = len(df_reviews_processed)\n",
    "        df_reviews_processed = df_reviews_processed[df_reviews_processed['cleaned_review'].str.len() >= MIN_REVIEW_LENGTH]\n",
    "        print(f\"Reviews: Removed {original_rows - len(df_reviews_processed)} rows with review shorter than {MIN_REVIEW_LENGTH} characters.\")\n",
    "    \n",
    "        # Handle 'anime_uid' type (IMPORTANT for joining)\n",
    "        # Ensure it's the same type as 'uid' in df_processed (int)\n",
    "        try:\n",
    "            df_reviews_processed['anime_uid'] = pd.to_numeric(df_reviews_processed['anime_uid'], errors='coerce')\n",
    "            df_reviews_processed.dropna(subset=['anime_uid'], inplace=True) # Remove if not convertible\n",
    "            df_reviews_processed['anime_uid'] = df_reviews_processed['anime_uid'].astype(int)\n",
    "        except Exception as e:\n",
    "             print(f\"Warning: Problem converting 'anime_uid' in df_reviews to numeric: {e}\")\n",
    "    \n",
    "        # Filter reviews to only include those for anime present in df_processed (optimization)\n",
    "        valid_anime_uids = df_processed['uid'].unique()\n",
    "        original_rows = len(df_reviews_processed)\n",
    "        df_reviews_processed = df_reviews_processed[df_reviews_processed['anime_uid'].isin(valid_anime_uids)]\n",
    "        print(f\"Reviews: Kept {len(df_reviews_processed)} reviews related to anime in the processed dataset (removed {original_rows - len(df_reviews_processed)}).\")\n",
    "\n",
    "        valid_anime_uids = df_processed['uid'].unique() # Use UIDs from the filtered anime set\n",
    "        original_rows = len(df_reviews_processed)\n",
    "        df_reviews_processed = df_reviews_processed[df_reviews_processed['anime_uid'].isin(valid_anime_uids)]\n",
    "        print(f\"Reviews: Kept {len(df_reviews_processed)} reviews related to anime in the *filtered* processed dataset (removed {original_rows - len(df_reviews_processed)}).\")\n",
    "    \n",
    "        # Reset index\n",
    "        df_reviews_processed.reset_index(drop=True, inplace=True)\n",
    "        df_reviews_processed.drop_duplicates(inplace=True)\n",
    "        df_reviews_processed.to_csv(PATH_PROCESSED_REVIEWS, index=False)\n",
    "        print(f\"Processed Reviews Dataset: {df_reviews_processed.shape[0]} reviews ready.\")\n",
    "        print(df_reviews_processed[['anime_uid', 'cleaned_review']].head())\n",
    "    \n",
    "    elif df_reviews.empty:\n",
    "        print(\"Reviews Dataset is empty or not loaded. Recommendations will be based on synopses only.\")\n",
    "    else: # df_processed is empty\n",
    "         print(\"Anime Dataset is empty, cannot process reviews meaningfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82eb586f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:42:06.905167Z",
     "iopub.status.busy": "2025-04-19T09:42:06.904740Z",
     "iopub.status.idle": "2025-04-19T09:42:06.917468Z",
     "shell.execute_reply": "2025-04-19T09:42:06.916293Z"
    },
    "papermill": {
     "duration": 0.023504,
     "end_time": "2025-04-19T09:42:06.919273",
     "exception": false,
     "start_time": "2025-04-19T09:42:06.895769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicates into the df_processed \n",
    "len(df_processed['uid'].astype(str).tolist()) == len(set(df_processed['uid'].astype(str).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da8618ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:42:06.937101Z",
     "iopub.status.busy": "2025-04-19T09:42:06.936748Z",
     "iopub.status.idle": "2025-04-19T09:42:06.942835Z",
     "shell.execute_reply": "2025-04-19T09:42:06.941803Z"
    },
    "papermill": {
     "duration": 0.0168,
     "end_time": "2025-04-19T09:42:06.944414",
     "exception": false,
     "start_time": "2025-04-19T09:42:06.927614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB directory not found at /kaggle/working/anime_chroma_db, no need to remove.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "chroma_db_path = \"/kaggle/working/anime_chroma_db\" # Make sure this matches the path used in PersistentClient\n",
    "\n",
    "if os.path.exists(chroma_db_path):\n",
    "    print(f\"Removing existing ChromaDB directory: {chroma_db_path}\")\n",
    "    shutil.rmtree(chroma_db_path)\n",
    "    print(\"Directory removed.\")\n",
    "else:\n",
    "    print(f\"ChromaDB directory not found at {chroma_db_path}, no need to remove.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "425bc7af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:42:06.962597Z",
     "iopub.status.busy": "2025-04-19T09:42:06.962138Z",
     "iopub.status.idle": "2025-04-19T09:45:59.461512Z",
     "shell.execute_reply": "2025-04-19T09:45:59.459881Z"
    },
    "papermill": {
     "duration": 232.510826,
     "end_time": "2025-04-19T09:45:59.463680",
     "exception": false,
     "start_time": "2025-04-19T09:42:06.952854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Semantic Embedding Generation & Storage (ChromaDB) ---\n",
      "Initializing ChromaDB PersistentClient at: /kaggle/working/anime_chroma_db\n",
      "GeminiEmbeddingFunction initialized with model: models/text-embedding-004, task_type: RETRIEVAL_DOCUMENT\n",
      "Getting or creating ChromaDB collection: anime_synopses\n",
      "Collection 'anime_synopses' ready. Current count: 0\n",
      "\n",
      "Adding documents to ChromaDB collection...\n",
      "Including metadata fields: ['title', 'genre', 'score', 'popularity']\n",
      "Found 0 existing IDs in the collection.\n",
      "Adding 7686 new documents to the collection (this will trigger embedding generation)...\n",
      "Adding ChromaDB batch 1...\n",
      "Embedding batch 1/5...\n",
      "Embedding batch 2/5...\n",
      "Embedding batch 3/5...\n",
      "Embedding batch 4/5...\n",
      "Embedding batch 5/5...\n",
      "Generated 500 embeddings.\n",
      "Adding ChromaDB batch 2...\n",
      "Embedding batch 1/5...\n",
      "Embedding batch 2/5...\n",
      "Embedding batch 3/5...\n",
      "Embedding batch 4/5...\n",
      "Embedding batch 5/5...\n",
      "Generated 500 embeddings.\n",
      "Adding ChromaDB batch 3...\n",
      "Embedding batch 1/5...\n",
      "Embedding batch 2/5...\n",
      "Embedding batch 3/5...\n",
      "Embedding batch 4/5...\n",
      "Embedding batch 5/5...\n",
      "Generated 500 embeddings.\n",
      "Adding ChromaDB batch 4...\n",
      "Embedding batch 1/5...\n",
      "Embedding batch 2/5...\n",
      "Embedding batch 3/5...\n",
      "Embedding batch 4/5...\n",
      "Embedding batch 5/5...\n",
      "Generated 500 embeddings.\n",
      "Adding ChromaDB batch 5...\n",
      "Embedding batch 1/5...\n",
      "Embedding batch 2/5...\n",
      "Embedding batch 3/5...\n",
      "Embedding batch 4/5...\n",
      "Embedding batch 5/5...\n",
      "Generated 500 embeddings.\n",
      "Adding ChromaDB batch 6...\n",
      "Embedding batch 1/5...\n",
      "Embedding batch 2/5...\n",
      "Embedding batch 3/5...\n",
      "Embedding batch 4/5...\n",
      "Embedding batch 5/5...\n",
      "Generated 500 embeddings.\n",
      "Adding ChromaDB batch 7...\n",
      "Embedding batch 1/5...\n",
      "Embedding batch 2/5...\n",
      "Embedding batch 3/5...\n",
      "Embedding batch 4/5...\n",
      "Embedding batch 5/5...\n",
      "Generated 500 embeddings.\n",
      "Adding ChromaDB batch 8...\n",
      "Embedding batch 1/5...\n",
      "Embedding batch 2/5...\n",
      "Embedding batch 3/5...\n",
      "Embedding batch 4/5...\n",
      "Embedding batch 5/5...\n",
      "Generated 500 embeddings.\n",
      "Adding ChromaDB batch 9...\n",
      "Embedding batch 1/5...\n",
      "Embedding batch 2/5...\n",
      "Embedding batch 3/5...\n",
      "Embedding batch 4/5...\n",
      "Embedding batch 5/5...\n",
      "Generated 500 embeddings.\n",
      "Adding ChromaDB batch 10...\n",
      "Embedding batch 1/5...\n",
      "Embedding batch 2/5...\n",
      "Embedding batch 3/5...\n",
      "Embedding batch 4/5...\n",
      "Embedding batch 5/5...\n",
      "Generated 500 embeddings.\n",
      "Adding ChromaDB batch 11...\n",
      "Embedding batch 1/5...\n",
      "Embedding batch 2/5...\n",
      "Embedding batch 3/5...\n",
      "Embedding batch 4/5...\n",
      "Embedding batch 5/5...\n",
      "Generated 500 embeddings.\n",
      "Adding ChromaDB batch 12...\n",
      "Embedding batch 1/5...\n",
      "Embedding batch 2/5...\n",
      "Embedding batch 3/5...\n",
      "Embedding batch 4/5...\n",
      "Embedding batch 5/5...\n",
      "Generated 500 embeddings.\n",
      "Adding ChromaDB batch 13...\n",
      "Embedding batch 1/5...\n",
      "Embedding batch 2/5...\n",
      "Embedding batch 3/5...\n",
      "Embedding batch 4/5...\n",
      "Embedding batch 5/5...\n",
      "Generated 500 embeddings.\n",
      "Adding ChromaDB batch 14...\n",
      "Embedding batch 1/5...\n",
      "Embedding batch 2/5...\n",
      "Embedding batch 3/5...\n",
      "Embedding batch 4/5...\n",
      "Embedding batch 5/5...\n",
      "Generated 500 embeddings.\n",
      "Adding ChromaDB batch 15...\n",
      "Embedding batch 1/5...\n",
      "Embedding batch 2/5...\n",
      "Embedding batch 3/5...\n",
      "Embedding batch 4/5...\n",
      "Embedding batch 5/5...\n",
      "Generated 500 embeddings.\n",
      "Adding ChromaDB batch 16...\n",
      "Embedding batch 1/2...\n",
      "Embedding batch 2/2...\n",
      "Generated 186 embeddings.\n",
      "Finished adding documents. Added: 7686, Failed: 0\n",
      "Total documents in collection: 7686\n",
      "Time taken for adding: 231.44 seconds.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4. Semantic Embedding Generation & Storage (Using ChromaDB)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 4. Semantic Embedding Generation & Storage (ChromaDB) ---\")\n",
    "\n",
    "# Define a helper to retry when per-minute quota is reached.\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "# --- 4.1 Define the Custom Gemini Embedding Function for ChromaDB ---\n",
    "class GeminiEmbeddingFunction(embedding_functions.EmbeddingFunction):\n",
    "    \"\"\"\n",
    "    Custom embedding function for ChromaDB using Google Gemini API.\n",
    "    Handles batching and potential retries.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = f\"models/{EMBEDDING_MODEL_NAME}\", task_type: str = \"RETRIEVAL_DOCUMENT\", api_key: str = None):\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API Key must be provided for GeminiEmbeddingFunction\")\n",
    "        \n",
    "        self._model_name = model_name\n",
    "        self._task_type = task_type\n",
    "        print(f\"GeminiEmbeddingFunction initialized with model: {self._model_name}, task_type: {self._task_type}\")\n",
    "\n",
    "    # Add the retry decorator for robustness\n",
    "    @retry.Retry(predicate=is_retriable)\n",
    "    def _embed_batch(self, batch_texts: list[str]) -> list:\n",
    "        \"\"\"Embeds a single batch of texts, handling API calls and errors.\"\"\"\n",
    "        try:\n",
    "            # Filter out empty strings, as they can cause API errors\n",
    "            valid_texts = [text for text in batch_texts if isinstance(text, str) and text.strip()]\n",
    "            if not valid_texts:\n",
    "                # Return list of Nones matching original batch size if all were invalid\n",
    "                return [None] * len(batch_texts)\n",
    "\n",
    "            # Call the Gemini API\n",
    "            response = client.models.embed_content(\n",
    "                model=self._model_name,\n",
    "                contents=valid_texts,\n",
    "                config=types.EmbedContentConfig(\n",
    "                    task_type=self._task_type,\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            # Create a map from valid text to its embedding\n",
    "            embedding_map = {text: emb for text, emb in zip(valid_texts, [e.values for e in response.embeddings])}\n",
    "            \n",
    "            # Return embeddings in the original order, inserting None for invalid texts\n",
    "            final_embeddings = [embedding_map.get(text) for text in batch_texts]\n",
    "            return final_embeddings\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error embedding batch with {len(batch_texts)} texts: {e}. Retrying if applicable...\")\n",
    "            # Reraise the exception to allow the @retry decorator to work\n",
    "            raise e\n",
    "\n",
    "    def __call__(self, input: list[str]) -> list[list[float]]:\n",
    "        \"\"\"\n",
    "        Embeds a list of documents using batching and rate limiting.\n",
    "        Expected input: list of strings.\n",
    "        Expected output: list of embeddings (list of floats).\n",
    "        \"\"\"\n",
    "        all_embeddings = []\n",
    "        batch_size = 100 # Gemini API batch limit (adjust if needed)\n",
    "        total_texts = len(input)\n",
    "\n",
    "        for i in range(0, total_texts, batch_size):\n",
    "            batch_texts = input[i:i + batch_size]\n",
    "            \n",
    "            # Add delay BETWEEN batches to respect per-minute limits\n",
    "            if i > 0:\n",
    "                time.sleep(1.0) # Wait 1 second between batch API calls\n",
    "\n",
    "            print(f\"Embedding batch {i//batch_size + 1}/{(total_texts + batch_size - 1)//batch_size}...\")\n",
    "            batch_embeddings = self._embed_batch(batch_texts)\n",
    "            \n",
    "            # Handle potential Nones returned from _embed_batch (if a text failed permanently)\n",
    "            # Replace None with zero vectors of the correct dimension if possible\n",
    "            # For simplicity here, we'll assume _embed_batch succeeds or raises an error handled by retry.\n",
    "            # A more robust implementation would determine dim and replace Nones.\n",
    "            # If the entire batch fails repeatedly, the retry deadline will be hit.\n",
    "            if any(emb is None for emb in batch_embeddings):\n",
    "                 print(f\"Warning: Some embeddings in batch {i//batch_size + 1} failed and are None.\")\n",
    "                 # Decide on a strategy: raise error, replace with zeros, or filter out.\n",
    "                 # For now, let's filter them out along with their original texts,\n",
    "                 # but this means the final list might be shorter than the input list.\n",
    "                 # A better approach for db.add might be to skip adding failed ones.\n",
    "                 # We will return them as None for now, ChromaDB might handle it.\n",
    "                 pass # Keep Nones for now\n",
    "\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "        # Check if all embeddings are None (total failure)\n",
    "        if all(e is None for e in all_embeddings):\n",
    "             raise RuntimeError(\"Failed to generate any embeddings.\")\n",
    "\n",
    "        # ChromaDB expects a list of lists of floats. Handle Nones if necessary.\n",
    "        # If ChromaDB cannot handle None, you MUST replace them with zero vectors here.\n",
    "        # Let's try returning as is first.\n",
    "        # Example replacement (if needed):\n",
    "        # first_valid_emb = next((e for e in all_embeddings if e is not None), None)\n",
    "        # if first_valid_emb:\n",
    "        #    dim = len(first_valid_emb)\n",
    "        #    all_embeddings = [e if e is not None else [0.0] * dim for e in all_embeddings]\n",
    "        # else:\n",
    "        #    raise RuntimeError(\"No valid embeddings generated to determine dimension.\")\n",
    "\n",
    "        print(f\"Generated {len(all_embeddings)} embeddings.\")\n",
    "        return all_embeddings\n",
    "\n",
    "\n",
    "# --- 4.2 Initialize ChromaDB Client and Collection ---\n",
    "print(f\"Initializing ChromaDB PersistentClient at: {chroma_db_path}\")\n",
    "chroma_client = chromadb.PersistentClient(path=chroma_db_path)\n",
    "\n",
    "# Define the embedding function instance (requires API Key)\n",
    "if GOOGLE_API_KEY:\n",
    "    gemini_ef = GeminiEmbeddingFunction(api_key=GOOGLE_API_KEY, task_type=\"RETRIEVAL_DOCUMENT\") # Use DOCUMENT type for indexing\n",
    "\n",
    "    collection_name = \"anime_synopses\"\n",
    "    print(f\"Getting or creating ChromaDB collection: {collection_name}\")\n",
    "    # Pass the embedding function instance to the collection\n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=gemini_ef,\n",
    "        metadata={\"hnsw:space\": \"cosine\"} # Specify cosine distance (good practice)\n",
    "    )\n",
    "    print(f\"Collection '{collection_name}' ready. Current count: {collection.count()}\")\n",
    "else:\n",
    "    print(\"ERROR: GOOGLE_API_KEY not set. Cannot initialize GeminiEmbeddingFunction for ChromaDB.\")\n",
    "    collection = None # Set collection to None if setup failed\n",
    "\n",
    "# --- 4.3 Add Documents to ChromaDB (if collection is ready and data exists) ---\n",
    "# This replaces the NumPy matrix generation loop\n",
    "\n",
    "if collection is not None and not df_processed.empty:\n",
    "    print(\"\\nAdding documents to ChromaDB collection...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Prepare data for ChromaDB\n",
    "    # We need lists of documents, ids, and optional metadatas\n",
    "    documents_to_add = df_processed['cleaned_synopsis'].tolist()\n",
    "    # Use the anime 'uid' as the ChromaDB ID (must be string)\n",
    "    ids_to_add = df_processed['uid'].astype(str).tolist()\n",
    "\n",
    "    # Create metadata (store title, genre, score, popularity for filtering/re-ranking)\n",
    "    # --- ENSURE 'score' and 'popularity' ARE INCLUDED HERE ---\n",
    "    metadata_columns = ['title', 'genre', 'score', 'popularity'] # Add 'popularity' or other relevant columns\n",
    "    # Check if columns exist in df_processed\n",
    "    valid_metadata_columns = [col for col in metadata_columns if col in df_processed.columns]\n",
    "    print(f\"Including metadata fields: {valid_metadata_columns}\")\n",
    "    if 'score' not in valid_metadata_columns or 'popularity' not in valid_metadata_columns:\n",
    "         print(\"WARNING: 'score' or 'popularity' column not found in df_processed. Re-ranking might not work as expected.\")\n",
    "\n",
    "    metadatas_to_add = df_processed[valid_metadata_columns].to_dict('records')\n",
    "\n",
    "    # --- IMPORTANT ---\n",
    "    # Handle potential NaN or non-numeric types in score/popularity before adding to metadata\n",
    "    # ChromaDB metadata values should ideally be simple types (string, int, float).\n",
    "    for meta_dict in metadatas_to_add:\n",
    "        if 'score' in meta_dict:\n",
    "            # Replace NaN score with a default (e.g., 0 or average) or handle as needed\n",
    "            meta_dict['score'] = float(meta_dict['score']) if pd.notna(meta_dict['score']) else 0.0\n",
    "        if 'popularity' in meta_dict:\n",
    "             # Ensure popularity is a number, replace NaN if necessary\n",
    "             meta_dict['popularity'] = int(meta_dict['popularity']) if pd.notna(meta_dict['popularity']) else 0\n",
    "\n",
    "    # Check if documents already exist (based on IDs) to avoid duplicates if re-running\n",
    "    existing_ids = set(collection.get(ids=ids_to_add)['ids'])\n",
    "    print(f\"Found {len(existing_ids)} existing IDs in the collection.\")\n",
    "\n",
    "    # Filter out data that already exists\n",
    "    new_documents = []\n",
    "    new_ids = []\n",
    "    new_metadatas = []\n",
    "    for doc, id_val, meta in zip(documents_to_add, ids_to_add, metadatas_to_add):\n",
    "        if id_val not in existing_ids:\n",
    "            new_documents.append(doc)\n",
    "            new_ids.append(id_val)\n",
    "            new_metadatas.append(meta)\n",
    "\n",
    "    if new_documents:\n",
    "        print(f\"Adding {len(new_documents)} new documents to the collection (this will trigger embedding generation)...\")\n",
    "        \n",
    "        # Add data in batches (ChromaDB handles calling the embedding function)\n",
    "        # The embedding function itself has internal batching for the API calls.\n",
    "        # ChromaDB's add also benefits from batching the add operation itself.\n",
    "        chroma_batch_size = 500 # How many items to add to ChromaDB at once\n",
    "        added_count = 0\n",
    "        failed_ids = []\n",
    "\n",
    "        for i in range(0, len(new_documents), chroma_batch_size):\n",
    "            print(f\"Adding ChromaDB batch {i//chroma_batch_size + 1}...\")\n",
    "            batch_docs = new_documents[i:i + chroma_batch_size]\n",
    "            batch_ids = new_ids[i:i + chroma_batch_size]\n",
    "            batch_metas = new_metadatas[i:i + chroma_batch_size]\n",
    "            \n",
    "            try:\n",
    "                 # This call will trigger the GeminiEmbeddingFunction internally\n",
    "                collection.add(\n",
    "                    documents=batch_docs,\n",
    "                    ids=batch_ids,\n",
    "                    metadatas=batch_metas\n",
    "                )\n",
    "                added_count += len(batch_docs)\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR adding ChromaDB batch starting at index {i}: {e}\")\n",
    "                # Log failed IDs for potential retry or investigation\n",
    "                failed_ids.extend(batch_ids)\n",
    "                # Decide whether to continue with next batch or stop\n",
    "                # continue\n",
    "\n",
    "        print(f\"Finished adding documents. Added: {added_count}, Failed: {len(failed_ids)}\")\n",
    "        print(f\"Total documents in collection: {collection.count()}\")\n",
    "        print(f\"Time taken for adding: {time.time() - start_time:.2f} seconds.\")\n",
    "    else:\n",
    "        print(\"No new documents to add. Collection is up-to-date.\")\n",
    "\n",
    "    # Clean up memory if df_processed is large and embeddings are now in ChromaDB\n",
    "    # del df_processed['embedding'] # Remove the old embedding column if it exists\n",
    "    # Note: We might still need df_processed for metadata if not fully stored in Chroma\n",
    "\n",
    "elif collection is None:\n",
    "    print(\"ChromaDB collection not initialized. Skipping document adding.\")\n",
    "else: # df_processed is empty\n",
    "    print(\"Processed Anime Dataset is empty. Skipping document adding to ChromaDB.\")\n",
    "\n",
    "# We no longer need the NumPy embeddings_matrix\n",
    "embeddings_matrix = None # Set to None to indicate it's not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a868bed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:45:59.492026Z",
     "iopub.status.busy": "2025-04-19T09:45:59.491646Z",
     "iopub.status.idle": "2025-04-19T09:45:59.513019Z",
     "shell.execute_reply": "2025-04-19T09:45:59.511647Z"
    },
    "papermill": {
     "duration": 0.037942,
     "end_time": "2025-04-19T09:45:59.515087",
     "exception": false,
     "start_time": "2025-04-19T09:45:59.477145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Semantic Retrieval Implementation (ChromaDB + Re-ranking) ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 5. Semantic Retrieval Implementation (Using ChromaDB with Re-ranking)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 5. Semantic Retrieval Implementation (ChromaDB + Re-ranking) ---\")\n",
    "\n",
    "# Define weights for re-ranking (these should sum to 1)\n",
    "# Adjust these based on experimentation and desired outcome\n",
    "W_SIMILARITY = 0.3\n",
    "W_SCORE = 0.1\n",
    "W_POPULARITY = 0.6 # Lower weight for popularity unless desired otherwise\n",
    "\n",
    "# Number of initial candidates to fetch for re-ranking\n",
    "INITIAL_K = 100 # Fetch more candidates (e.g., 25)\n",
    "\n",
    "def find_similar_anime_chromadb(query, collection, top_k=TOP_K_RESULTS):\n",
    "    \"\"\"Finds anime most similar to a query using ChromaDB.\"\"\"\n",
    "    if collection is None:\n",
    "         print(\"Error: ChromaDB collection is not available.\")\n",
    "         return pd.DataFrame() # Return empty DataFrame\n",
    "\n",
    "    print(f\"\\nQuerying ChromaDB for: '{query}'...\")\n",
    "    try:\n",
    "        # ChromaDB uses the collection's embedding function to embed the query\n",
    "        # By default, it uses the same task_type (\"RETRIEVAL_DOCUMENT\")\n",
    "        # For potentially better results, embed query separately with \"RETRIEVAL_QUERY\"\n",
    "        # query_embedding = get_embedding(query, task_type=\"RETRIEVAL_QUERY\") # Use the single embedding func\n",
    "        # if query_embedding is None: raise ValueError(\"Failed to embed query\")\n",
    "        # results = collection.query(query_embeddings=[query_embedding], n_results=top_k, include=['metadatas', 'distances'])\n",
    "\n",
    "        # Simpler approach: Let ChromaDB handle query embedding with its default EF\n",
    "        results = collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=top_k,\n",
    "            include=['metadatas', 'distances'] # Request metadata and distances\n",
    "            # You could add a 'where' clause here for filtering if needed\n",
    "            # where={\"genre\": {\"$like\": \"%Slice of Life%\"}}\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying ChromaDB: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Process ChromaDB results into a DataFrame similar to the previous one\n",
    "    if not results or not results.get('ids') or not results['ids'][0]:\n",
    "        print(\"No results found in ChromaDB for this query.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    ids = results['ids'][0]\n",
    "    distances = results['distances'][0]\n",
    "    metadatas = results['metadatas'][0]\n",
    "\n",
    "    # Convert distance to similarity (Cosine distance d = 1 - s => similarity s = 1 - d)\n",
    "    similarities = [1 - d for d in distances]\n",
    "\n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'uid': ids, # ChromaDB IDs are strings, match original type if needed later\n",
    "        'similarity': similarities,\n",
    "        # Extract metadata fields\n",
    "        'title': [m.get('title', 'N/A') for m in metadatas],\n",
    "        'genre': [m.get('genre', 'N/A') for m in metadatas],\n",
    "        'score': [m.get('score', np.nan) for m in metadatas]\n",
    "        # Add other metadata fields if stored\n",
    "    })\n",
    "    \n",
    "    # Convert uid back to int if needed for joins later (assuming original uid was int)\n",
    "    try:\n",
    "        results_df['uid'] = results_df['uid'].astype(int)\n",
    "    except ValueError:\n",
    "        print(\"Warning: Could not convert ChromaDB string IDs back to integer.\")\n",
    "\n",
    "\n",
    "    print(f\"Found {len(results_df)} results from ChromaDB (Top {top_k}).\")\n",
    "    return results_df\n",
    "\n",
    "def find_similar_anime_chromadb_reranked(query, collection, top_k=TOP_K_RESULTS, k_initial=INITIAL_K):\n",
    "    \"\"\"\n",
    "    Finds anime most similar to a query using ChromaDB, then re-ranks\n",
    "    the initial results based on similarity, score, and popularity.\n",
    "    \"\"\"\n",
    "    if collection is None:\n",
    "         print(\"Error: ChromaDB collection is not available.\")\n",
    "         return pd.DataFrame()\n",
    "\n",
    "    print(f\"\\nQuerying ChromaDB for initial {k_initial} candidates for: '{query}'...\")\n",
    "    try:\n",
    "        # Query ChromaDB for more initial candidates\n",
    "        results = collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=k_initial, # Fetch more results initially\n",
    "            include=['metadatas', 'distances'] # Ensure metadata (score, pop) is included\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying ChromaDB: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Process ChromaDB results\n",
    "    if not results or not results.get('ids') or not results['ids'][0]:\n",
    "        print(f\"No results found in ChromaDB for this query (initial fetch).\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    ids = results['ids'][0]\n",
    "    distances = results['distances'][0]\n",
    "    metadatas = results['metadatas'][0]\n",
    "\n",
    "    # Convert distance to similarity\n",
    "    similarities = [1 - d for d in distances]\n",
    "\n",
    "    # Create DataFrame with initial results\n",
    "    initial_results_df = pd.DataFrame({\n",
    "        'uid': ids,\n",
    "        'similarity': similarities,\n",
    "        'title': [m.get('title', 'N/A') for m in metadatas],\n",
    "        'genre': [m.get('genre', 'N/A') for m in metadatas],\n",
    "        # --- Extract score and popularity ---\n",
    "        # Use .get with a default (e.g., 0 or NaN) if metadata might be missing\n",
    "        'score': [m.get('score', 0.0) for m in metadatas],\n",
    "        'popularity': [m.get('popularity', 0) for m in metadatas]\n",
    "    })\n",
    "\n",
    "    # --- Re-ranking Phase ---\n",
    "    print(f\"Re-ranking the initial {len(initial_results_df)} candidates...\")\n",
    "\n",
    "    # Handle potential missing data before normalization (though defaults in .get should handle it)\n",
    "    initial_results_df['score'].fillna(0.0, inplace=True)\n",
    "    initial_results_df['popularity'].fillna(0, inplace=True)\n",
    "\n",
    "    # Normalize features (Similarity, Score, Popularity) to 0-1 range\n",
    "    scaler = MinMaxScaler()\n",
    "    # Prepare data for scaling (handle cases with only 1 result where min=max)\n",
    "    features_to_scale = []\n",
    "    if initial_results_df['similarity'].nunique() > 1:\n",
    "        features_to_scale.append('similarity')\n",
    "    else: # Handle constant value case\n",
    "         initial_results_df['norm_similarity'] = 0.5 # Assign a neutral value or 1.0\n",
    "\n",
    "    if initial_results_df['score'].nunique() > 1:\n",
    "         features_to_scale.append('score')\n",
    "    else:\n",
    "         initial_results_df['norm_score'] = 0.5\n",
    "\n",
    "    # --- Popularity (Lower is better) ---\n",
    "    # No log transform needed if it's already a rank\n",
    "    if initial_results_df['popularity'].nunique() > 1:\n",
    "        features_to_scale.append('popularity')\n",
    "    else:\n",
    "         initial_results_df['norm_popularity'] = 0.5\n",
    "\n",
    "\n",
    "    if features_to_scale:\n",
    "        scaled_features = scaler.fit_transform(initial_results_df[features_to_scale])\n",
    "        # Create normalized columns\n",
    "        for i, feature in enumerate(features_to_scale):\n",
    "             initial_results_df[f'norm_{feature.replace(\"log_\", \"\")}'] = scaled_features[:, i] # Assign back to df\n",
    "\n",
    "    # Calculate combined score using weights\n",
    "    # Ensure normalized columns exist before calculating\n",
    "    norm_sim = initial_results_df.get('norm_similarity', 0.5) # Default if column doesn't exist\n",
    "    norm_score = initial_results_df.get('norm_score', 0.5)\n",
    "    norm_pop = initial_results_df.get('norm_popularity', 0.5)\n",
    "\n",
    "    initial_results_df['combined_score'] = (W_SIMILARITY * norm_sim +\n",
    "                                            W_SCORE * norm_score +\n",
    "                                            W_POPULARITY * (1 - norm_pop)) # Invert popularity contribution\n",
    "\n",
    "    # Sort by the combined score in descending order\n",
    "    reranked_df = initial_results_df.sort_values(by='combined_score', ascending=False)\n",
    "\n",
    "    # Select the final top_k results\n",
    "    final_results_df = reranked_df.head(top_k).copy()\n",
    "\n",
    "    # Convert uid back to int if needed for joins later\n",
    "    try:\n",
    "        final_results_df['uid'] = final_results_df['uid'].astype(int)\n",
    "    except ValueError:\n",
    "        print(\"Warning: Could not convert ChromaDB string IDs back to integer in final results.\")\n",
    "\n",
    "    print(f\"Re-ranking complete. Returning top {len(final_results_df)} results.\")\n",
    "    # Optionally display the combined score for debugging/analysis\n",
    "    # print(final_results_df[['uid', 'title', 'similarity', 'score', 'popularity', 'combined_score']].round(4).to_string(index=False))\n",
    "\n",
    "    return final_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "284bb15e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:45:59.543438Z",
     "iopub.status.busy": "2025-04-19T09:45:59.543025Z",
     "iopub.status.idle": "2025-04-19T09:45:59.555487Z",
     "shell.execute_reply": "2025-04-19T09:45:59.554132Z"
    },
    "papermill": {
     "duration": 0.028797,
     "end_time": "2025-04-19T09:45:59.557333",
     "exception": false,
     "start_time": "2025-04-19T09:45:59.528536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. Response Generation with LLM (RAG) ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 6. Response Generation with LLM (RAG: Synopsis + Reviews)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 6. Response Generation with LLM (RAG) ---\")\n",
    "\n",
    "def generate_recommendations(query, context_df, reviews_df, model_name=GENERATIVE_MODEL_NAME, reviews_per_anime=REVIEWS_PER_ANIME):\n",
    "    \"\"\"Generates motivated recommendations using LLM, retrieved context (synopsis), and reviews.\"\"\"\n",
    "\n",
    "    if not GOOGLE_API_KEY:\n",
    "        return \"API Key not available. Cannot generate recommendation.\"\n",
    "\n",
    "    # context_df here is the DataFrame returned by find_similar_anime_chromadb\n",
    "    # *after* merging it with df_processed to get the 'cleaned_synopsis'\n",
    "    if context_df.empty:\n",
    "        return \"No relevant context provided to generate recommendations.\"\n",
    "\n",
    "    context_text_for_prompt = \"\"\n",
    "    print(\"\\nPreparing context for LLM (including reviews)...\")\n",
    "    for index, row in context_df.iterrows():\n",
    "        # Ensure column names match the DataFrame passed to this function\n",
    "        try:\n",
    "            anime_id = row['uid'] # Should be present after merge\n",
    "            title = row['title'] # Should be present from Chroma metadata or merge\n",
    "            synopsis = row['cleaned_synopsis'] # CRUCIAL: Must be present after merge\n",
    "            similarity = row['similarity'] # Present from Chroma results\n",
    "        except KeyError as e:\n",
    "            print(f\"ERROR: Missing expected column in context_df for LLM: {e}\")\n",
    "            continue # Skip this entry if essential data is missing\n",
    "\n",
    "        review_snippets = \"No relevant reviews found in the processed dataset.\"\n",
    "        if not reviews_df.empty:\n",
    "            # Retrieve reviews for this anime using 'anime_uid' from the reviews dataframe\n",
    "            # Ensure 'anime_id' (which is the 'uid' from context_df) is the correct type for matching\n",
    "            try:\n",
    "                # Ensure reviews_df['anime_uid'] is the same type as anime_id (e.g., int)\n",
    "                anime_reviews = reviews_df[reviews_df['anime_uid'] == int(anime_id)] # Explicit cast if needed\n",
    "                if not anime_reviews.empty:\n",
    "                    selected_reviews = anime_reviews['cleaned_review'].dropna().head(reviews_per_anime).tolist()\n",
    "                    if selected_reviews:\n",
    "                        review_snippets = \"\\n\".join([f\"- \\\"{rev[:350]}...\\\"\" for rev in selected_reviews])\n",
    "            except Exception as e:\n",
    "                 print(f\"Warning: Error retrieving reviews for anime_id {anime_id}: {e}\")\n",
    "\n",
    "\n",
    "        # Add the information to the context for the prompt\n",
    "        context_text_for_prompt += f\"--- Retrieved Anime ---\\n\"\n",
    "        context_text_for_prompt += f\"ID: {anime_id}\\n\"\n",
    "        context_text_for_prompt += f\"Title: {title}\\n\"\n",
    "        context_text_for_prompt += f\"Synopsis Similarity to Query: {similarity:.4f}\\n\"\n",
    "        context_text_for_prompt += f\"Synopsis: {synopsis}\\n\"\n",
    "        context_text_for_prompt += f\"User Review Excerpts:\\n{review_snippets}\\n\\n\"\n",
    "\n",
    "    if not context_text_for_prompt:\n",
    "         return \"Could not construct any valid context for the LLM.\"\n",
    "\n",
    "    # Build the updated prompt\n",
    "    prompt = f\"\"\"You are an expert and empathetic anime and manga advisor. A user expresses a desire based on a feeling, inspiration, or atmosphere:\n",
    "\"{query}\"\n",
    "\n",
    "Based EXCLUSIVELY on the following retrieved information (titles, synopses, synopsis similarity to query, and excerpts of user reviews), suggest 2 or 3 anime/manga from the provided list.\n",
    "For each suggestion:\n",
    "1.  State the Title.\n",
    "2.  Briefly explain why you think it might match the user's requested feeling/inspiration. Refer to BOTH the synopsis (for plot/theme) AND the user impressions in the reviews (for atmosphere/perceived emotional impact).\n",
    "3.  Be concise, engaging, and direct. Do not invent information not present in the provided context.\n",
    "\n",
    "Here is the retrieved information:\n",
    "{context_text_for_prompt}\n",
    "---\n",
    "\n",
    "Your 2-3 motivated recommendations:\n",
    "\"\"\"\n",
    "\n",
    "    # Call the LLM\n",
    "    print(f\"Sending request to {model_name}...\")\n",
    "    try:\n",
    "        answer = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=prompt\n",
    "        )\n",
    "        return textwrap.fill(answer.text, width=90)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LLM response generation: {e}\")\n",
    "        try:\n",
    "            if 'response' in locals() and hasattr(response, 'prompt_feedback'):\n",
    "                 print(f\"Prompt Feedback: {response.prompt_feedback}\")\n",
    "            elif 'response' in locals() and hasattr(response, 'candidates') and response.candidates:\n",
    "                 print(f\"Candidate Finish Reason: {response.candidates[0].finish_reason}\")\n",
    "                 print(f\"Safety Ratings: {response.candidates[0].safety_ratings}\")\n",
    "        except Exception as feedback_e:\n",
    "            print(f\"Could not retrieve feedback details: {feedback_e}\")\n",
    "        return \"Sorry, I encountered a technical issue while generating the recommendation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fe2ec3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:45:59.585502Z",
     "iopub.status.busy": "2025-04-19T09:45:59.585063Z",
     "iopub.status.idle": "2025-04-19T09:45:59.595921Z",
     "shell.execute_reply": "2025-04-19T09:45:59.594386Z"
    },
    "papermill": {
     "duration": 0.027015,
     "end_time": "2025-04-19T09:45:59.597810",
     "exception": false,
     "start_time": "2025-04-19T09:45:59.570795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 7. Main Semantic Search Function (Using ChromaDB + Re-ranking) ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 7. Putting It All Together: The Main Search Function (Using ChromaDB + Re-ranking)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 7. Main Semantic Search Function (Using ChromaDB + Re-ranking) ---\")\n",
    "\n",
    "# Make sure 'collection' is globally accessible or passed correctly\n",
    "# If running sections independently, re-get the collection:\n",
    "# try:\n",
    "#     chroma_client = chromadb.PersistentClient(path=chroma_db_path)\n",
    "#     collection = chroma_client.get_collection(name=collection_name, embedding_function=gemini_ef) # Need EF again if client is recreated\n",
    "# except Exception as e:\n",
    "#     print(f\"Error re-getting collection: {e}\")\n",
    "#     collection = None\n",
    "\n",
    "def semantic_anime_search_engine(query, chroma_collection=collection, df_rev=df_reviews_processed):\n",
    "    \"\"\"Performs the complete semantic search using ChromaDB + Re-ranking + RAG.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"NEW SEMANTIC SEARCH (ChromaDB + Re-ranking) FOR: '{query}'\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Preliminary checks\n",
    "    if not GOOGLE_API_KEY:\n",
    "         print(\"ERROR: Google API Key not configured.\")\n",
    "         return \"API configuration missing.\"\n",
    "    if chroma_collection is None:\n",
    "         print(\"ERROR: ChromaDB collection is not available.\")\n",
    "         return \"Cannot perform search. ChromaDB collection missing.\"\n",
    "    # We still need reviews data\n",
    "    if df_rev.empty:\n",
    "         print(\"INFO: Reviews dataset is not available or empty. Recommendations will be based on synopses only.\")\n",
    "\n",
    "    # 1. Retrieve AND RE-RANK similar documents using ChromaDB\n",
    "    print(\"\\n--- Phase 1: Retrieval & Re-ranking (ChromaDB) ---\")\n",
    "    # --- CALL THE NEW RE-RANKING FUNCTION ---\n",
    "    #retrieved_results = find_similar_anime_chromadb(query, chroma_collection)\n",
    "    retrieved_results = find_similar_anime_chromadb_reranked(query, chroma_collection, top_k=TOP_K_RESULTS, k_initial=INITIAL_K)\n",
    "\n",
    "    if retrieved_results.empty:\n",
    "        print(\"\\n--- Final Result ---\")\n",
    "        print(\"I couldn't find any similar anime/manga in the ChromaDB collection for your request after re-ranking.\")\n",
    "        print(\"=\" * 60)\n",
    "        return \"No relevant results found in the retrieval/re-ranking phase.\"\n",
    "\n",
    "    print(f\"\\nDocuments retrieved and re-ranked (Top {TOP_K_RESULTS}):\")\n",
    "    # Display relevant info from the re-ranked DataFrame\n",
    "    print(retrieved_results[['uid', 'title', 'similarity', 'score', 'popularity', 'combined_score']].round(4).to_string(index=False))\n",
    "\n",
    "    # 2. Fetch Synopses (Merge step - remains the same logic)\n",
    "    print(\"\\nFetching synopses for re-ranked results...\")\n",
    "    # ... (The merging code to get 'cleaned_synopsis' remains the same) ...\n",
    "    # Make sure df_processed is available\n",
    "    if 'df_processed' not in globals() or df_processed.empty:\n",
    "         print(\"ERROR: df_processed not available to fetch synopses for retrieved results.\")\n",
    "         return \"Cannot proceed without original data to fetch synopses.\"\n",
    "    try:\n",
    "         df_processed['uid'] = df_processed['uid'].astype(int) # Ensure type match\n",
    "         retrieved_results_with_synopsis = pd.merge(\n",
    "             retrieved_results,\n",
    "             df_processed[['uid', 'cleaned_synopsis']],\n",
    "             on='uid',\n",
    "             how='left'\n",
    "         )\n",
    "         retrieved_results_with_synopsis.dropna(subset=['cleaned_synopsis'], inplace=True) # Drop if merge failed for some\n",
    "         if retrieved_results_with_synopsis.empty: raise ValueError(\"No results after merge\")\n",
    "    except Exception as e:\n",
    "         print(f\"ERROR merging results with df_processed to get synopsis: {e}\")\n",
    "         return \"Failed to fetch synopses for re-ranked results.\"\n",
    "\n",
    "\n",
    "    # 3. Generate the response using the LLM (remains the same logic)\n",
    "    print(\"\\n--- Phase 2: Generation (Creating recommendation with LLM) ---\")\n",
    "    # Pass the re-ranked DataFrame (now including 'cleaned_synopsis')\n",
    "    final_recommendation = generate_recommendations(query, retrieved_results_with_synopsis, df_rev)\n",
    "\n",
    "    #print(\"\\n--- Final Generated Recommendation ---\")\n",
    "    #print(final_recommendation)\n",
    "    #print(\"=\" * 60)\n",
    "    return Markdown(final_recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a7b9823",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:45:59.625873Z",
     "iopub.status.busy": "2025-04-19T09:45:59.625420Z",
     "iopub.status.idle": "2025-04-19T09:46:01.716384Z",
     "shell.execute_reply": "2025-04-19T09:46:01.715076Z"
    },
    "papermill": {
     "duration": 2.107099,
     "end_time": "2025-04-19T09:46:01.718308",
     "exception": false,
     "start_time": "2025-04-19T09:45:59.611209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 8. Usage Examples (ChromaDB) ---\n",
      "============================================================\n",
      "NEW SEMANTIC SEARCH (ChromaDB + Re-ranking) FOR: 'a tear-jerking romantic story between a boy and a girl from different social classes'\n",
      "============================================================\n",
      "\n",
      "--- Phase 1: Retrieval & Re-ranking (ChromaDB) ---\n",
      "\n",
      "Querying ChromaDB for initial 100 candidates for: 'a tear-jerking romantic story between a boy and a girl from different social classes'...\n",
      "Embedding batch 1/1...\n",
      "Generated 1 embeddings.\n",
      "Re-ranking the initial 100 candidates...\n",
      "Re-ranking complete. Returning top 10 results.\n",
      "\n",
      "Documents retrieved and re-ranked (Top 10):\n",
      "  uid                      title  similarity  score  popularity  combined_score\n",
      " 5150          Hatsukoi Limited.      0.7972   7.37        1449          0.8365\n",
      "22839                 Cross Road      0.7800   7.46        2040          0.7572\n",
      "  345 Eikoku Koi Monogatari Emma      0.7802   7.72        2281          0.7522\n",
      "13833           Nagareboshi Lens      0.7901   6.71        3448          0.7303\n",
      " 1689      Byousoku 5 Centimeter      0.7460   7.86         108          0.7184\n",
      "34822             Tsuki ga Kirei      0.7470   8.27         395          0.7161\n",
      "  885           Tenshi no Tamago      0.7587   7.67        1498          0.7075\n",
      "30771                      Mudai      0.8024   7.31        6537          0.6645\n",
      "36214        Asagao to Kase-san.      0.7530   7.86        2426          0.6562\n",
      "  756            School Days ONA      0.7619   6.08        2672          0.6558\n",
      "\n",
      "Fetching synopses for re-ranked results...\n",
      "\n",
      "--- Phase 2: Generation (Creating recommendation with LLM) ---\n",
      "\n",
      "Preparing context for LLM (including reviews)...\n",
      "Sending request to gemini-2.0-flash...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Okay, based on your request for a tear-jerking romantic story between a boy and a girl\n",
       "from different social classes, here are a couple of anime suggestions:  1.  **Title:**\n",
       "Eikoku Koi Monogatari Emma     *   **Why:** This anime directly addresses the social class\n",
       "divide in 19th-century London. The synopsis describes a maid, Emma, falling for William, a\n",
       "member of the gentry. The reviews highlight the classic, potentially heartbreaking nature\n",
       "of the love story, with one reviewer directly comparing it to \"Romeo and Juliet.\" This\n",
       "suggests a strong potential for the tear-jerking element you're looking for. 2.\n",
       "**Title:** Mudai     *   **Why:** While not explicitly about social class, the synopsis\n",
       "speaks of a \"modest\" life for the artist and his girlfriend, followed by unprecedented\n",
       "success that seems to lead to the world crumbling around him. This has the potential to be\n",
       "a tear-jerking story about love tested by changing circumstances and the pressures of\n",
       "success, with emotional highs and lows."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 8. Usage Examples (Updated to use ChromaDB search function)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 8. Usage Examples (ChromaDB) ---\")\n",
    "\n",
    "# Example 1: Epic and Inspiring\n",
    "#semantic_anime_search_engine(\"I want something truly epic and inspiring, that gives me a huge emotional and visual charge\")\n",
    "\n",
    "# Example 2: Melancholic and Reflective\n",
    "#semantic_anime_search_engine(\"I'm looking for a melancholic, quiet, and reflective anime, maybe with a slightly mysterious or supernatural atmosphere\")\n",
    "\n",
    "# Example 3: Light Romantic Comedy\n",
    "#semantic_anime_search_engine(\"I need to unwind with a light, funny romantic comedy that makes me feel good\")\n",
    "\n",
    "# Example 4: Dark and Psychological\n",
    "#semantic_anime_search_engine(\"Recommend something dark, psychological, a bit unsettling, that makes you think about human nature\")\n",
    "\n",
    "# Example 5: Carefree Fantasy Adventure\n",
    "#semantic_anime_search_engine(\"A carefree adventure in a colorful fantasy world, to dream a little\")\n",
    "\n",
    "semantic_anime_search_engine(\"a tear-jerking romantic story between a boy and a girl from different social classes\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 97258,
     "sourceType": "competition"
    },
    {
     "datasetId": 465305,
     "sourceId": 874035,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 377.66106,
   "end_time": "2025-04-19T09:46:03.660025",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-19T09:39:45.998965",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
